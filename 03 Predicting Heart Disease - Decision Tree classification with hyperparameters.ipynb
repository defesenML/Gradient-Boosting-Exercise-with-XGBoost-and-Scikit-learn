{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e44757d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072cd6b7",
   "metadata": {},
   "source": [
    "Decision trees, the most commonly used XGBoost base learners, are unique in the machine learning landscape. Instead of multiplying column values by numeric weights, as in linear regression and logistic regression, decision trees split the data by asking questions about the columns.This process of splitting data into new groups via branching continues until the algorithm reaches a desired level of accuracy.\n",
    "Decision trees are prone to overfitting the data. In other words, decision trees can map too closely to the training data, a problem in terms of variance and bias. Hyperparameter fine-tuning is one solution to prevent overfitting. Another solution is to aggregate the predictions of many trees, a strategy that Random Forests and XGBoost employ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76204807",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b039c54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data (file was saved in local directory)\n",
    "df_census = pd.read_csv('census_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf72a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare your predictor and target columns, X and y:\n",
    "X = df_census.iloc[:,:-1]\n",
    "y = df_census.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5af2358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19a75018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n",
    "#random_state: choosing the seed of a pseudo-random number generator to ensure reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c95037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The accuracy_score determines the number of correct predictions divided by the total number of predictions. \n",
    "# import the DecisionTreeClassifier and accuracy_score:\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c93c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a machine learning model with random_state=2 to ensure consistent results:\n",
    "DecTree= DecisionTreeClassifier(random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96c0dad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model on the training set:\n",
    "DecTree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b4e4879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions for the test set:\n",
    "y_pred = DecTree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db370250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8131679154894976"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compare predictions with the test set:\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26524520",
   "metadata": {},
   "source": [
    "##### Gini Criterion:\n",
    "Gini is the error method the decision tree uses to decide how splits should be made. The goal is to find a split that leads to the lowest error. A Gini index of 0 means 0 errors. A gini index of 1 means all errors. A gini index of 0.5, which shows an equal distribution of elements, means the predictions are no better than random guessing. The closer to 0, the lower the error. At the root, a gini of 0.364 means the training set is imbalanced with 36.4 percent of class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21692d9a",
   "metadata": {},
   "source": [
    "##### Bias:\n",
    "A straight line generally has high bias. In machine learning bias is a mathematical term that comes from estimating the error when applying the model to a real-life problem. The bias of the straight line is high because the predictions are restricted to the line and fail to account for changes in the data.\n",
    "In many cases, a straight line is not complex enough to make accurate predictions. When this happens, we say that the machine learning model has underfit the data with high bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9ea729",
   "metadata": {},
   "source": [
    "##### Variance: \n",
    "In machine learning, variance is a mathematical term indicating how much a model will change given a different set of training data. Formally, variance is the measure of the squared deviation between a random variable and its mean. Given nine different data points in the training set, the eighth-degree polynomial will be completely different, resulting in high variance.\n",
    "Models with high variance often overfit the data. These models do not generalize well to new data points because they have fit the training data too closely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34210e",
   "metadata": {},
   "source": [
    "#### Low bias, low variance:\n",
    "Low variance means that a different training set will not result in a curve that differs by a significant amount. Low bias indicates that the error when applying this model to a real-world situation will not be too high. In machine learning, the combination of low variance and low bias is ideal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d838ca",
   "metadata": {},
   "source": [
    "## Tuning decision tree hyperparameters\n",
    "\n",
    "In machine learning, parameters are adjusted when the model is being tuned. The weights in linear and Logistic Regression, for example, are parameters adjusted during the build phase to minimize errors. Hyperparameters, by contrast, are chosen in advance of the build phase. If no hyperparameters are selected, default values are used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b66916",
   "metadata": {},
   "source": [
    "#### Decision Tree Regressor\n",
    "Before selecting hyperparameters, let's start by finding a baseline score using a DecisionTreeRegressor and cross_val_score with the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "942b2120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the 'bike_rentals_cleaned' dataset and split it into X_bikes (predictor columns) and y_bikes (training columns):\n",
    "df_bikes = pd.read_csv('bike_rentals_cleaned.csv')\n",
    "X_bikes = df_bikes.iloc[:,:-1]\n",
    "y_bikes = df_bikes.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "675a8d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the DecisionTreeRegressor and cross_val_score:\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd349065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DecisionTreeRegressor and fit the model in cross_val_score:\n",
    "reg = DecisionTreeRegressor(random_state=2)\n",
    "\n",
    "scores = cross_val_score(reg, X_bikes, y_bikes, scoring='neg_mean_squared_error', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0b33c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE mean: 1233.36\n"
     ]
    }
   ],
   "source": [
    "# Compute the root mean squared error (RMSE) and print the results:\n",
    "rmse = np.sqrt(-scores)\n",
    "\n",
    "print('RMSE mean: %0.2f' % (rmse.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2369d83",
   "metadata": {},
   "source": [
    "###### The RMSE is 1233.36. This is worse than the 972.06 obtained from Linear Regression, and from the 887.31 obtained by XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c105df",
   "metadata": {},
   "source": [
    "Is the model overfitting the data because the variance is too high?\n",
    "We can check how well decision tree makes predictions on training set alone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a50d2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f0bf98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bikes, y_bikes, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcc84609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The following code checks the error of the training set, before it makes predictions on the test set:\n",
    "reg = DecisionTreeRegressor()\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error \n",
    "reg_mse = mean_squared_error(y_train, y_pred)\n",
    "reg_rmse = np.sqrt(reg_mse)\n",
    "reg_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85565555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A RMSE of 0.0 means that the model has perfectly fit every data point! \n",
    "#This perfect score combined with a cross-validation error of 1233.36 is proof that the decision tree is overfitting the data with high variance. \n",
    "#The training set fit perfectly, but the test set missed badly.\n",
    "\n",
    "# Hyperparameters may rectify the situation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b73a066",
   "metadata": {},
   "source": [
    "##### max_depth:\n",
    "max_depth defines the depth of the tree, determined by the number of times splits are made.By limiting max_depth to smaller numbers, variance is reduced, and the model generalizes better to new data.\n",
    "\n",
    "How can you choose the best number for max_depth?\n",
    "\n",
    "You can always try max_depth=1, then max_depth=2, then max_depth=3, and so on, but this process would be exhausting. Instead, you may use a tool called GridSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780c3466",
   "metadata": {},
   "source": [
    "##### GridSearchCV:\n",
    "GridSearchCV searches a grid of hyperparameters using cross-validation to deliver the best results.\n",
    "\n",
    "GridSearchCV functions as any machine learning algorithm, meaning that it's fit on a training set, and scored on a test set. The primary difference is that GridSearchCV checks all hyperparameters before finalizing a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a5924fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import GridSearchCV and define a list of hyperparameters for max_depth as follows:\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "params = {'max_depth':[None,2,3,4,6,8,10,20]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2b07b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=DecisionTreeRegressor(random_state=2), n_jobs=-1,\n",
       "             param_grid={'max_depth': [None, 2, 3, 4, 6, 8, 10, 20]},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize a DecisionTreeRegressor, and place it inside of GridSearchCV along with params and the scoring metric:\n",
    "reg = DecisionTreeRegressor(random_state=2)\n",
    "grid_reg = GridSearchCV(reg, params, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
    "grid_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fe8b0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': 6}\n"
     ]
    }
   ],
   "source": [
    "best_params = grid_reg.best_params_\n",
    "print(\"Best params:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87850932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 951.398\n"
     ]
    }
   ],
   "source": [
    "# training score:\n",
    "best_score = np.sqrt(-grid_reg.best_score_)\n",
    "print(\"Training score: {:.3f}\".format(best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0cde082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 864.670\n"
     ]
    }
   ],
   "source": [
    "# test score:\n",
    "best_model = grid_reg.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse_test = mean_squared_error(y_test, y_pred)**0.5\n",
    "print('Test score: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e58c1a1",
   "metadata": {},
   "source": [
    "##### min_samples_leaf\n",
    "min_samples_leaf provides a restriction by increasing the number of samples that a leaf may have. As with max_depth, min_samples_leaf is designed to reduce overfitting. Increasing min_samples_leaf reduces variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3cb1297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that displays the best parameters, training score, and test score using GridSearchCV with DecisionTreeRegressor(random_state=2) assigned to reg as a default parameter:\n",
    "def grid_search(params, reg=DecisionTreeRegressor(random_state=2)):\n",
    "    grid_reg = GridSearchCV(reg, params, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
    "    grid_reg.fit(X_train, y_train)\n",
    "    best_params = grid_reg.best_params_    \n",
    "    print(\"Best params:\", best_params)    \n",
    "    best_score = np.sqrt(-grid_reg.best_score_)    \n",
    "    print(\"Training score: {:.3f}\".format(best_score))\n",
    "\n",
    "    y_pred = grid_reg.predict(X_test)    \n",
    "    rmse_test = mean_squared_error(y_test, y_pred)**0.5\n",
    "    print('Test score: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6fc386c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(548, 12)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0eabb74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'min_samples_leaf': 8}\n",
      "Training score: 896.083\n",
      "Test score: 855.620\n"
     ]
    }
   ],
   "source": [
    "#Let's try [1, 2, 4, 6, 8, 10, 20, 30] as the input of our grid_search:\n",
    "grid_search(params={'min_samples_leaf':[1, 2, 4, 6, 8, 10, 20, 30]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b949501c",
   "metadata": {},
   "source": [
    "###### Since the test score is better than the training score, variance has been reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76f91488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': 6, 'min_samples_leaf': 2}\n",
      "Training score: 870.396\n",
      "Test score: 913.000\n"
     ]
    }
   ],
   "source": [
    "#put min_samples_leaf and max_depth together:\n",
    "grid_search(params={'max_depth':[None,2,3,4,6,8,10,20],'min_samples_leaf':[1,2,4,6,8,10,20,30]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cfe6aa",
   "metadata": {},
   "source": [
    "##### Even though the training score has improved, the test score has not. min_samples_leaf has decreased from 8 to 2, while max_depth has remained the same. Hyperparameters should not be chosen in isolation!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5307ba49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': 9, 'min_samples_leaf': 7}\n",
      "Training score: 888.905\n",
      "Test score: 878.538\n"
     ]
    }
   ],
   "source": [
    "#limit min_samples_leaf to values greater than 3:\n",
    "grid_search(params={'max_depth':[6,7,8,9,10],'min_samples_leaf':[3,5,7,9]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821514ea",
   "metadata": {},
   "source": [
    "###### the test score has improved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a89073",
   "metadata": {},
   "source": [
    "max_leaf_nodes\n",
    "max_leaf_nodes is similar to min_samples_leaf. Instead of specifying the number of samples per leaf, it specifies the total number of leaves. So, max_leaf_nodes=10 means that the model cannot have more than 10 leaves. It could have fewer.\n",
    "\n",
    "max_features\n",
    "max_features is an effective hyperparameter for reducing variance. Instead of considering every possible feature for a split, it chooses from a select number of features each round.\n",
    "It's standard to see max_features with the following options:\n",
    "\n",
    "'auto' is the default, which provides no limitations.\n",
    "'sqrt' is the square root of the total number of features.\n",
    "'log2' is the log of the total number of features in base 2. 32 columns resolves to 5 since 2 ^5 = 32.\n",
    "\n",
    "min_samples_split\n",
    "Another splitting technique is min_samples_split. As the name indicates, min_samples_split provides a limit to the number of samples required before a split can be made. The default is 2, since two samples may be split into one sample each, ending as single leaves. If the limit is increased to 5, no further splits are permitted for nodes with five samples or fewer.\n",
    "\n",
    "splitter\n",
    "There are two options for splitter, 'random' and 'best'. Splitter tells the model how to select the feature to split each branch. The 'best' option, the default, selects the feature that results in the greatest gain of information. The 'random' option, by contrast, selects the split randomly.\n",
    "\n",
    "Changing splitter to 'random' is a great way to prevent overfitting and diversify trees.\n",
    "\n",
    "criterion\n",
    "The criterion for splitting decision tree regressors and classifiers are different. The criterion provides the method the machine learning model uses to determine how splits should be made. It's the scoring method for splits. For each possible split, the criterion calculates a number for a possible split and compares it to other options. The split with the best score wins.\n",
    "\n",
    "The options for decision tree regressors are mse (mean squared error), friedman_mse, (which includes Friedman's adjustment), and mae (mean absolute error). The default is mse.  \n",
    "\n",
    "For classifiers, gini, which was described earlier, and entropy usually give similar results.\n",
    "\n",
    "min_impurity_decrease\n",
    "Previously known as min_impurity_split, min_impurity_decrease results in a split when the impurity is greater than or equal to this value.\n",
    "\n",
    "Impurity is a measure of how pure the predictions are for every node. A tree with 100% accuracy would have an impurity of 0.0. A tree with 80% accuracy would have an impurity of 0.20.\n",
    "\n",
    "Impurity is an important idea in Decision Trees. Throughout the tree-building process, impurity should continually decrease. Splits that result in the greatest decrease of impurity are chosen for each node.\n",
    "\n",
    "The default value is 0.0. This number can be increased so that trees stop building when a certain threshold is reached.\n",
    "\n",
    "min_weight_fraction_leaf\n",
    "min_weight_fraction_leaf is the minimum weighted fraction of the total weights required to be a leaf. According to the documentation, Samples have equal weight when sample_weight is not provided.\n",
    "\n",
    "For practical purposes, min_weight_fraction_leaf is another hyperparameter that reduces variance and prevents overfitting. The default is 0.0. Assuming equal weights, a restriction of 1%, 0.01, would require at least 5 of the 500 samples to be a leaf. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a727ebc",
   "metadata": {},
   "source": [
    "## Predicting heart disease – a case study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346dfe5c",
   "metadata": {},
   "source": [
    "Develop a model and highlight two to three important features that doctors and nurses can focus on to improve patient health.\n",
    "Use a decision tree classifier with fine-tuned hyperparameters. \n",
    "After the model has been built, interpret results using feature_importances_, an attribute that determines the most important features in predicting heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "296b9be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data (file was saved in local directory)\n",
    "df_heart = pd.read_csv('heart_disease.csv')\n",
    "df_heart.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a2dcc",
   "metadata": {},
   "source": [
    "'target' is binary, with 1 indicating that the patient has heart disease and 0 indicating that they do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b3f5a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets:\n",
    "X = df_heart.iloc[:,:-1]\n",
    "y = df_heart.iloc[:,-1]\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3b3236",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b529413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the DecisionTreeClassifier and accuracy_score:\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60c262a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [0.74 0.85 0.77 0.73 0.7 ]\n",
      "Accuracy mean: 0.76\n"
     ]
    }
   ],
   "source": [
    "#Use cross_val_score with a DecisionTreeClassifier:\n",
    "model = DecisionTreeClassifier(random_state=2)\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print('Accuracy:', np.round(scores, 2))\n",
    "print('Accuracy mean: %0.2f' % (scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d9e2ac",
   "metadata": {},
   "source": [
    "#### RandomizedSearchCV\n",
    "\n",
    "RandomizedSearchCV works in the same way as GridSearchCV, but instead of trying all hyperparameters, it tries a random number of combinations. It's meant to find the best combinations in limited time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d280a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that uses RandomizedSearchCV to return the best model along with the scores:\n",
    "# The inputs are params (a dictionary of hyperparameters to test), runs (number of hyperparameter combinations to check), and DecisionTreeClassifier\n",
    "def randomized_search_clf(params, runs=20, clf=DecisionTreeClassifier(random_state=2)):    \n",
    "    rand_clf = RandomizedSearchCV(clf, params, n_iter=runs, cv=5, n_jobs=-1, random_state=2)    \n",
    "    rand_clf.fit(X_train, y_train)\n",
    "    best_model = rand_clf.best_estimator_\n",
    "    best_score = rand_clf.best_score_  \n",
    "    print(\"Training score: {:.3f}\".format(best_score))\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Test score: {:.3f}'.format(accuracy))\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029739fb",
   "metadata": {},
   "source": [
    "##### Choosing hyperparameters\n",
    "Numbers have been chosen with the aim of reducing variance and trying an expansive range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d8537fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "51d4728a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.798\n",
      "Test score: 0.855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=8, max_features=0.8,\n",
       "                       max_leaf_nodes=45, min_samples_leaf=0.04,\n",
       "                       min_samples_split=10, min_weight_fraction_leaf=0.05,\n",
       "                       random_state=2)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomized_search_clf(params={'criterion':['entropy', 'gini'],'splitter':['random', 'best'], 'min_weight_fraction_leaf':[0.0, 0.0025, 0.005, 0.0075, 0.01],'min_samples_split':[2, 3, 4, 5, 6, 8, 10],'min_samples_leaf':[1, 0.01, 0.02, 0.03, 0.04],'min_impurity_decrease':[0.0, 0.0005, 0.005, 0.05, 0.10, 0.15, 0.2],'max_leaf_nodes':[10, 15, 20, 25, 30, 35, 40, 45, 50, None],'max_features':['auto', 0.95, 0.90, 0.85, 0.80, 0.75, 0.70],'max_depth':[None, 2,4,6,8],'min_weight_fraction_leaf':[0.0, 0.0025, 0.005, 0.0075, 0.01, 0.05]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54484ac8",
   "metadata": {},
   "source": [
    "##### Narrowing the range\n",
    "Narrowing the range is one strategy to improve hyperparameters.\n",
    "Using a baseline of max_depth=8 chosen from the best model, we may narrow the range to from 7 to 9.\n",
    "Another strategy is to stop checking hyperparameters whose defaults are working fine. entropy, for instance, is not recommended over 'gini' as the differences are very slight. min_impurity_split and min_impurity_decrease may also be left at their defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "47233899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.802\n",
      "Test score: 0.868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=7, max_features=0.78, max_leaf_nodes=45,\n",
       "                       min_samples_leaf=0.045, min_samples_split=9,\n",
       "                       min_weight_fraction_leaf=0.06, random_state=2)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomized_search_clf(params={'max_depth':[None, 6, 7],'max_features':['auto', 0.78], 'max_leaf_nodes':[45, None], 'min_samples_leaf':[1, 0.035, 0.04, 0.045, 0.05],'min_samples_split':[2, 9, 10],'min_weight_fraction_leaf': [0.0, 0.05, 0.06, 0.07],}, runs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3e83a60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [0.82 0.9  0.8  0.8  0.78]\n",
      "Accuracy mean: 0.82\n"
     ]
    }
   ],
   "source": [
    "# For a proper baseline of comparison,  it's essential to put the new model into cross_val_clf.\n",
    "model = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7, max_features=0.78, max_leaf_nodes=45, min_impurity_decrease=0.0,  min_samples_leaf=0.045, min_samples_split=9, min_weight_fraction_leaf=0.06,  random_state=2, splitter='best')\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print('Accuracy:', np.round(scores, 2))\n",
    "print('Accuracy mean: %0.2f' % (scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ced19e",
   "metadata": {},
   "source": [
    "##### six percentage points higher than the default model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31da074e",
   "metadata": {},
   "source": [
    "#### Some further trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "349c051f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.802\n",
      "Test score: 0.868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=10, max_features=0.8, max_leaf_nodes=45,\n",
       "                       min_samples_leaf=0.06, min_samples_split=6,\n",
       "                       random_state=2)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomized_search_clf(params={'max_depth':[7, 8, 9,10],'max_features':['auto', 0.80], 'max_leaf_nodes':[45, None],  'min_impurity_decrease': [0.0, 0.1], 'min_samples_leaf':[1, 0.06, 0.07],'min_samples_split':[4,6,8,10],'min_weight_fraction_leaf': [0.0, 0.05], 'splitter': ['best'],}, runs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5857d090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [0.82 0.9  0.8  0.8  0.78]\n",
      "Accuracy mean: 0.82\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10, max_features=0.8, max_leaf_nodes=45, min_samples_leaf=0.06,   min_samples_split=6,  random_state=2)\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print('Accuracy:', np.round(scores, 2))\n",
    "print('Accuracy mean: %0.2f' % (scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7736c857",
   "metadata": {},
   "source": [
    "### feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e24dca59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=9, max_features=0.8, max_leaf_nodes=47,\n",
       "                       min_samples_split=8, min_weight_fraction_leaf=0.05,\n",
       "                       random_state=2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=9,max_features=0.8, max_leaf_nodes=47,min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=8,min_weight_fraction_leaf=0.05, random_state=2, splitter='best')\n",
    "\n",
    "best_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "baef5c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04830121, 0.04008887, 0.47546568, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00976578, 0.        , 0.02445397,\n",
       "       0.02316427, 0.1774694 , 0.20129082])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "43f602e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cp', 0.47546567857183675),\n",
       " ('thal', 0.20129082387838435),\n",
       " ('ca', 0.1774694042213901)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_dict = dict(zip(X.columns, best_clf.feature_importances_))\n",
    "\n",
    "# Import operator \n",
    "import operator\n",
    "\n",
    "#Sort dict by values (as list of tuples)\n",
    "sorted(feature_dict.items(), key=operator.itemgetter(1), reverse=True)[0:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1808aec",
   "metadata": {},
   "source": [
    "'cp': Chest pain type (1 = typical angina, 2 = atypical angina, 3 = non-anginal pain, 4 = asymptomatic)\n",
    "'thalach': Maximum heart rate achieved\n",
    "'ca': Number of major vessels (0-3) colored by fluoroscopy\n",
    "\n",
    "These numbers may be interpreted as their explanation of variance, so 'cp' accounts for 48% of the variance, which is more than 'thal' and 'ca' combined."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
