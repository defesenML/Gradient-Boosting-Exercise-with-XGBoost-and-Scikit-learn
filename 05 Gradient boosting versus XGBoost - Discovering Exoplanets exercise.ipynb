{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6141c6cf",
   "metadata": {},
   "source": [
    "The general idea behind boosting algorithms is to transform weak learners into strong learners. A weak learner is hardly better than random guessing. But there is a purpose behind the weak start. Building on this general idea, boosting works by focusing on iterative error correction, not by establishing a strong baseline model. If the base model is too strong, the learning process is necessarily limited, thereby undermining the general strategy behind boosting models.\n",
    "\n",
    "Weak learners are transformed into strong learners through hundreds of iterations. Boosting has been one of the best general machine learning strategies in terms of producing optimal results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b7d984",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "While gradient boosting also adjusts based on incorrect predictions (similar to AdaBoost), it takes this idea one step further: gradient boosting fits each new tree entirely based on the errors of the previous tree's predictions. That is, for each new tree, gradient boosting looks at the mistakes and then builds a new tree completely around these mistakes. The new tree doesn't care about the predictions that are already correct.\n",
    "Gradient boosting computes the residuals of each tree's predictions and sums all the residuals to score the model.\n",
    "A gradient boosting model is built from scratch by training new trees on the errors of the previous trees. \n",
    "We continue with the bike rentals dataset to compare new models with the previous models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1875a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50f170bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>0.590435</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>1562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.226957</td>\n",
       "      <td>0.229270</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant  season   yr  mnth  holiday  weekday  workingday  weathersit  \\\n",
       "0        1     1.0  0.0     1      0.0      6.0         0.0           2   \n",
       "1        2     1.0  0.0     1      0.0      0.0         0.0           2   \n",
       "2        3     1.0  0.0     1      0.0      1.0         1.0           1   \n",
       "3        4     1.0  0.0     1      0.0      2.0         1.0           1   \n",
       "4        5     1.0  0.0     1      0.0      3.0         1.0           1   \n",
       "\n",
       "       temp     atemp       hum  windspeed   cnt  \n",
       "0  0.344167  0.363625  0.805833   0.160446   985  \n",
       "1  0.363478  0.353739  0.696087   0.248539   801  \n",
       "2  0.196364  0.189405  0.437273   0.248309  1349  \n",
       "3  0.200000  0.212122  0.590435   0.160296  1562  \n",
       "4  0.226957  0.229270  0.436957   0.186900  1600  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the bike rental dataset:\n",
    "df_bikes = pd.read_csv('bike_rentals_cleaned.csv')\n",
    "df_bikes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f53195",
   "metadata": {},
   "source": [
    "#### Building a gradient boosting model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2412a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into X and y. Then, split X and y into training and test sets:\n",
    "X_bikes = df_bikes.iloc[:,:-1]\n",
    "y_bikes = df_bikes.iloc[:,-1]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bikes, y_bikes, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4cd20ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the data to the decision tree:  The initial decision tree, called a base learner, should not be fine-tuned for accuracy. \n",
    "# We want a model that focuses on learning from errors, not a model that relies heavily on the base learner.\n",
    "# Initialize a decision tree with max_depth=2 and fit it on the training set as tree_1, since it's the first tree in our ensemble:\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_1 = DecisionTreeRegressor(max_depth=2, random_state=2)\n",
    "tree_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f53293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions with the training set: Instead of making predictions with the test set, predictions in gradient boosting are initially made with the training set. \n",
    "#To compute the residuals, we need to compare the predictions while still in the training phase. The test phase of the model build comes at the end, after all the trees have been constructed. \n",
    "#The predictions of the training set for the first round are obtained by adding the predict method to tree_1 with X_train as the input:\n",
    "y_train_pred = tree_1.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0184095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the residuals: The residuals are the differences between the predictions and the target column. \n",
    "# The predictions of X_train, defined here as y_train_pred, are subtracted from y_train, the target column, to compute the residuals:\n",
    "y2_train = y_train - y_train_pred\n",
    "# The residuals are defined as y2_train because they are the new target column for the next tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f633c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the new tree on the residuals: Fitting a new tree on the residuals is different than fitting a model on the training set. \n",
    "#The primary difference is in the predictions. In the bike rentals dataset, when fitting a new tree on the residuals, we should progressively get smaller numbers.\n",
    "#Initialize a new tree and fit it on X_train and the residuals, y2_train:\n",
    "tree_2 = DecisionTreeRegressor(max_depth=2, random_state=2)\n",
    "tree_2.fit(X_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46d49557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repeat steps 2-4: As the process continues, the residuals should gradually approach 0 from the positive and negative direction. \n",
    "#The iterations continue for the number of estimators, n_estimators.\n",
    "# Let's repeat the process for a third tree as follows:\n",
    "y2_train_pred = tree_2.predict(X_train)\n",
    "y3_train = y2_train - y2_train_pred\n",
    "tree_3 = DecisionTreeRegressor(max_depth=2, random_state=2)\n",
    "tree_3.fit(X_train, y3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "072ce9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a learning purpose, stop the process here.\n",
    "# Sum the results: Summing the results requires making predictions for each tree with the test set as follows:\n",
    "y1_pred = tree_1.predict(X_test)\n",
    "y2_pred = tree_2.predict(X_test)\n",
    "y3_pred = tree_3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3df26af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y1_pred + y2_pred + y3_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71788df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "911.0479538776444"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lastly, let's compute the mean squared error (MSE) to obtain the results as follows:\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "MSE(y_test, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c7593d",
   "metadata": {},
   "source": [
    "#### Building a gradient boosting model in scikit-learn\n",
    "\n",
    "Use GradientBoostingRegressor with hyperparameter adjustments. The advantage of using GradientBoostingRegressor is that it's much faster to build and easier to implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "938092ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the regressor from the sklearn.ensemble library:\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4826923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To obtain the same results, it's essential to match max_depth=2 and random_state=2. \n",
    "# Since there are only three trees, we must have n_estimators=3. \n",
    "# Finally, we must set the learning_rate=1.0 hyperparameter.\n",
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=3, random_state=2, learning_rate=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c475d730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "911.0479538776439"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the model has been initialized, it can be fit on the training data and scored against the test data:\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "MSE(y_test, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94df27a8",
   "metadata": {},
   "source": [
    "The result is the same to 11 decimal places.\n",
    "Recall that the point of gradient boosting is to build a model with enough trees to transform a weak learner into a strong learner. This is easily done by changing n_estimators, the number of iterations, to a much larger number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "023bf0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "857.1072323426944"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's build and score a gradient boosting regressor with 30 estimators:\n",
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=30, random_state=2, learning_rate=1.0)\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "MSE(y_test, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e55928f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "936.3617413678853"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when n_estimators = 300:\n",
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=300, random_state=2, learning_rate=1.0)\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "MSE(y_test, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be318d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "653.7456840231495"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove parameter learning_rate:\n",
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=300, random_state=2)\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "MSE(y_test, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2821f1a",
   "metadata": {},
   "source": [
    "### Modifying GB Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208cf0e9",
   "metadata": {},
   "source": [
    "learning_rate, also known as the shrinkage, shrinks the contribution of individual trees so that no tree has too much influence when building the model. If an entire ensemble is built from the errors of one base learner, without careful adjustment of hyperparameters, early trees in the model can have too much influence on subsequent development. learning_rate limits the influence of individual trees. Generally speaking, as n_estimators, the number of trees, goes up, learning_rate should go down.\n",
    "learning_rate ranges from 0 to 1. A learning_rate value of 1 means that no adjustments are made. The default value of 0.1 means that the tree's influence is weighted at 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c705417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.001 , Score: 1633.0261400367258\n",
      "Learning Rate: 0.01 , Score: 831.5430182728547\n",
      "Learning Rate: 0.05 , Score: 685.0192988749717\n",
      "Learning Rate: 0.1 , Score: 653.7456840231495\n",
      "Learning Rate: 0.15 , Score: 687.666134269379\n",
      "Learning Rate: 0.2 , Score: 664.312804425697\n",
      "Learning Rate: 0.3 , Score: 689.4190385930236\n",
      "Learning Rate: 0.5 , Score: 693.8856905068778\n",
      "Learning Rate: 1.0 , Score: 936.3617413678853\n"
     ]
    }
   ],
   "source": [
    "# build and score a new GradientBoostingRegressor to see how the scores compare:\n",
    "learning_rate_values = [0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5, 1.0]\n",
    "\n",
    "for value in learning_rate_values:\n",
    "    gbr = GradientBoostingRegressor(max_depth=2,   n_estimators=300, random_state=2, learning_rate=value)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    y_pred = gbr.predict(X_test)\n",
    "    rmse = MSE(y_test, y_pred)**0.5\n",
    "    print('Learning Rate:', value, ', Score:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d67d5c",
   "metadata": {},
   "source": [
    "##### the default learning_rate value of 0.1 gives the best score for 300 trees.It's important to tune learning_rate and n_estimators together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3877a639",
   "metadata": {},
   "source": [
    "Base learner\n",
    "The initial decision tree in the gradient boosting regressor is called the base learner because it's at the base of the ensemble. It's the first learner in the process. It is a weak learner transforming into a strong learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df477ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth: None , Score: 869.2788645118395\n",
      "Max Depth: 1 , Score: 707.8261886858736\n",
      "Max Depth: 2 , Score: 653.7456840231495\n",
      "Max Depth: 3 , Score: 646.4045923317708\n",
      "Max Depth: 4 , Score: 663.048387855927\n"
     ]
    }
   ],
   "source": [
    "#It's possible to tune base learners for gains in accuracy.\n",
    "# select a max_depth value of 1, 2, 3, or 4 and compare results as follows:\n",
    "depths = [None, 1, 2, 3, 4]\n",
    "\n",
    "for depth in depths:\n",
    "    gbr = GradientBoostingRegressor(max_depth=depth, n_estimators=300, random_state=2)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    y_pred = gbr.predict(X_test)\n",
    "    rmse = MSE(y_test, y_pred)**0.5\n",
    "    print('Max Depth:', depth, ', Score:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e3a334",
   "metadata": {},
   "source": [
    "subsample \n",
    "is a subset of samples. Since samples are the rows, a subset of rows means that all rows may not be included when building each tree. By changing subsample from 1.0 to a smaller decimal, trees only select that percentage of samples during the build phase. For example, subsample=0.8 would select 80% of samples for each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89b68fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsample: 1 , Score: 646.4045923317708\n",
      "Subsample: 0.9 , Score: 620.1819001443569\n",
      "Subsample: 0.8 , Score: 617.2355650565677\n",
      "Subsample: 0.7 , Score: 612.9879156983139\n",
      "Subsample: 0.6 , Score: 622.6385116402317\n",
      "Subsample: 0.5 , Score: 626.9974073227554\n"
     ]
    }
   ],
   "source": [
    "samples = [1, 0.9, 0.8, 0.7, 0.6, 0.5]\n",
    "\n",
    "for sample in samples:\n",
    "    gbr = GradientBoostingRegressor(max_depth=3, n_estimators=300, subsample=sample, random_state=2)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    y_pred = gbr.predict(X_test)\n",
    "    rmse = MSE(y_test, y_pred)**0.5\n",
    "    print('Subsample:', sample, ', Score:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce61d13b",
   "metadata": {},
   "source": [
    "RandomizedSearchCV\n",
    "Our preliminary analysis indicated that a grid search centered around max_depth=3, subsample=0.7, n_estimators=300, and learning_rate = 0.1 is a good place to start. As n_estimators goes up, learning_rate should go down..RandomizedSearchCV is preferred over GridSearchCV when there is too many combinations of hyperparameters. It will speed up computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f7a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting point:\n",
    "params={'subsample':[0.65, 0.7, 0.75],\n",
    "        'n_estimators':[300, 500, 1000],\n",
    "         'learning_rate':[0.05, 0.075, 0.1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bbfb152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import RandomizedSearchCV and initialize a gradient boosting model:\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "gbr = GradientBoostingRegressor(max_depth=3, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f434ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RandomizedSearchCV with gbr and params as inputs in addition to the number of iterations, the scoring, and the number of folds. \n",
    "# Recall that n_jobs=-1 may speed up computations and random_state=2 ensures the consistency of results:\n",
    "rand_reg = RandomizedSearchCV(gbr, params, n_iter=10, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8baf5029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'subsample': 0.65, 'n_estimators': 300, 'learning_rate': 0.05}\n",
      "Training score: 636.200\n",
      "Test set score: 625.985\n"
     ]
    }
   ],
   "source": [
    "# fit the model on the training set and obtain the best parameters and scores:\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "rand_reg.fit(X_train, y_train)\n",
    "best_model = rand_reg.best_estimator_\n",
    "best_params = rand_reg.best_params_\n",
    "\n",
    "print(\"Best params:\", best_params)\n",
    "best_score = np.sqrt(-rand_reg.best_score_)\n",
    "print(\"Training score: {:.3f}\".format(best_score))\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse_test = MSE(y_test, y_pred)**0.5\n",
    "print('Test set score: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9c4fe85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596.9544588974487"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after experimentations, a better model is possible:\n",
    "gbr = GradientBoostingRegressor(max_depth=3, n_estimators=1600, subsample=0.75, learning_rate=0.02, random_state=2)\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "MSE(y_test, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd92cc4",
   "metadata": {},
   "source": [
    "XGBoost\n",
    "XGBoost is an advanced version of gradient boosting with the same general structure, it transforms weak learners into strong learners by summing the residuals of trees.\n",
    "The only difference in hyperparameters from the last section is that XGBoost refers to learning_rate as eta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39ef0f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584.3395337495713"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intialize the model:\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xg_reg = XGBRegressor(max_depth=3, n_estimators=1600, eta=0.02, subsample=0.75, random_state=2)\n",
    "xg_reg.fit(X_train, y_train)\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "MSE(y_test, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76833e4",
   "metadata": {},
   "source": [
    "##### XGBoost is preferred over gradient boosting in general because it consistently delivers better results, and because it's faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ab313e",
   "metadata": {},
   "source": [
    "### Gradient boosting versus XGBoost - exoplanet example\n",
    "\n",
    "We examine exoplanets over time. The dataset has 5,087 rows and 3,189 columns that record light flux at different times of a star's life cycle. Multiplying columns and rows together results in 1.5 million data points. Using a baseline of 100 trees, we need 150 million data points to build a model.\n",
    "Each row is an individual star and the columns reveal different light patterns over time. In addition to light patterns, an exoplanet column is labeled 2 if the star hosts an exoplanet; otherwise, it is labeled 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79f3dfdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>FLUX.1</th>\n",
       "      <th>FLUX.2</th>\n",
       "      <th>FLUX.3</th>\n",
       "      <th>FLUX.4</th>\n",
       "      <th>FLUX.5</th>\n",
       "      <th>FLUX.6</th>\n",
       "      <th>FLUX.7</th>\n",
       "      <th>FLUX.8</th>\n",
       "      <th>FLUX.9</th>\n",
       "      <th>...</th>\n",
       "      <th>FLUX.3188</th>\n",
       "      <th>FLUX.3189</th>\n",
       "      <th>FLUX.3190</th>\n",
       "      <th>FLUX.3191</th>\n",
       "      <th>FLUX.3192</th>\n",
       "      <th>FLUX.3193</th>\n",
       "      <th>FLUX.3194</th>\n",
       "      <th>FLUX.3195</th>\n",
       "      <th>FLUX.3196</th>\n",
       "      <th>FLUX.3197</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>93.85</td>\n",
       "      <td>83.81</td>\n",
       "      <td>20.10</td>\n",
       "      <td>-26.98</td>\n",
       "      <td>-39.56</td>\n",
       "      <td>-124.71</td>\n",
       "      <td>-135.18</td>\n",
       "      <td>-96.27</td>\n",
       "      <td>-79.89</td>\n",
       "      <td>...</td>\n",
       "      <td>-78.07</td>\n",
       "      <td>-102.15</td>\n",
       "      <td>-102.15</td>\n",
       "      <td>25.13</td>\n",
       "      <td>48.57</td>\n",
       "      <td>92.54</td>\n",
       "      <td>39.32</td>\n",
       "      <td>61.42</td>\n",
       "      <td>5.08</td>\n",
       "      <td>-39.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-38.88</td>\n",
       "      <td>-33.83</td>\n",
       "      <td>-58.54</td>\n",
       "      <td>-40.09</td>\n",
       "      <td>-79.31</td>\n",
       "      <td>-72.81</td>\n",
       "      <td>-86.55</td>\n",
       "      <td>-85.33</td>\n",
       "      <td>-83.97</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.28</td>\n",
       "      <td>-32.21</td>\n",
       "      <td>-32.21</td>\n",
       "      <td>-24.89</td>\n",
       "      <td>-4.86</td>\n",
       "      <td>0.76</td>\n",
       "      <td>-11.70</td>\n",
       "      <td>6.46</td>\n",
       "      <td>16.00</td>\n",
       "      <td>19.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>532.64</td>\n",
       "      <td>535.92</td>\n",
       "      <td>513.73</td>\n",
       "      <td>496.92</td>\n",
       "      <td>456.45</td>\n",
       "      <td>466.00</td>\n",
       "      <td>464.50</td>\n",
       "      <td>486.39</td>\n",
       "      <td>436.56</td>\n",
       "      <td>...</td>\n",
       "      <td>-71.69</td>\n",
       "      <td>13.31</td>\n",
       "      <td>13.31</td>\n",
       "      <td>-29.89</td>\n",
       "      <td>-20.88</td>\n",
       "      <td>5.06</td>\n",
       "      <td>-11.80</td>\n",
       "      <td>-28.91</td>\n",
       "      <td>-70.02</td>\n",
       "      <td>-96.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>326.52</td>\n",
       "      <td>347.39</td>\n",
       "      <td>302.35</td>\n",
       "      <td>298.13</td>\n",
       "      <td>317.74</td>\n",
       "      <td>312.70</td>\n",
       "      <td>322.33</td>\n",
       "      <td>311.31</td>\n",
       "      <td>312.42</td>\n",
       "      <td>...</td>\n",
       "      <td>5.71</td>\n",
       "      <td>-3.73</td>\n",
       "      <td>-3.73</td>\n",
       "      <td>30.05</td>\n",
       "      <td>20.03</td>\n",
       "      <td>-12.67</td>\n",
       "      <td>-8.77</td>\n",
       "      <td>-17.31</td>\n",
       "      <td>-17.35</td>\n",
       "      <td>13.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-1107.21</td>\n",
       "      <td>-1112.59</td>\n",
       "      <td>-1118.95</td>\n",
       "      <td>-1095.10</td>\n",
       "      <td>-1057.55</td>\n",
       "      <td>-1034.48</td>\n",
       "      <td>-998.34</td>\n",
       "      <td>-1022.71</td>\n",
       "      <td>-989.57</td>\n",
       "      <td>...</td>\n",
       "      <td>-594.37</td>\n",
       "      <td>-401.66</td>\n",
       "      <td>-401.66</td>\n",
       "      <td>-357.24</td>\n",
       "      <td>-443.76</td>\n",
       "      <td>-438.54</td>\n",
       "      <td>-399.71</td>\n",
       "      <td>-384.65</td>\n",
       "      <td>-411.79</td>\n",
       "      <td>-510.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LABEL   FLUX.1   FLUX.2   FLUX.3   FLUX.4   FLUX.5   FLUX.6  FLUX.7  \\\n",
       "0      2    93.85    83.81    20.10   -26.98   -39.56  -124.71 -135.18   \n",
       "1      2   -38.88   -33.83   -58.54   -40.09   -79.31   -72.81  -86.55   \n",
       "2      2   532.64   535.92   513.73   496.92   456.45   466.00  464.50   \n",
       "3      2   326.52   347.39   302.35   298.13   317.74   312.70  322.33   \n",
       "4      2 -1107.21 -1112.59 -1118.95 -1095.10 -1057.55 -1034.48 -998.34   \n",
       "\n",
       "    FLUX.8  FLUX.9  ...  FLUX.3188  FLUX.3189  FLUX.3190  FLUX.3191  \\\n",
       "0   -96.27  -79.89  ...     -78.07    -102.15    -102.15      25.13   \n",
       "1   -85.33  -83.97  ...      -3.28     -32.21     -32.21     -24.89   \n",
       "2   486.39  436.56  ...     -71.69      13.31      13.31     -29.89   \n",
       "3   311.31  312.42  ...       5.71      -3.73      -3.73      30.05   \n",
       "4 -1022.71 -989.57  ...    -594.37    -401.66    -401.66    -357.24   \n",
       "\n",
       "   FLUX.3192  FLUX.3193  FLUX.3194  FLUX.3195  FLUX.3196  FLUX.3197  \n",
       "0      48.57      92.54      39.32      61.42       5.08     -39.54  \n",
       "1      -4.86       0.76     -11.70       6.46      16.00      19.93  \n",
       "2     -20.88       5.06     -11.80     -28.91     -70.02     -96.67  \n",
       "3      20.03     -12.67      -8.77     -17.31     -17.35      13.98  \n",
       "4    -443.76    -438.54    -399.71    -384.65    -411.79    -510.54  \n",
       "\n",
       "[5 rows x 3198 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Load the exoplanet dataset:\n",
    "df = pd.read_csv('exoplanets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f41eb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5087 entries, 0 to 5086\n",
      "Columns: 3198 entries, LABEL to FLUX.3197\n",
      "dtypes: float64(3197), int64(1)\n",
      "memory usage: 124.1 MB\n"
     ]
    }
   ],
   "source": [
    "# are all columns numerical?\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6de10dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of null values?\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7c55d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and test sets. \n",
    "#Note that the 0th column is the target column, y, and all other columns are the predictor columns, X:\n",
    "X = df.iloc[:,1:]\n",
    "y = df.iloc[:,0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2596b919",
   "metadata": {},
   "source": [
    "#### Building gradient boosting classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4da7e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import GradientBoostingClassifer and XGBClassifier in addition to accuracy_score so that we may compare both models:\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4d9f38",
   "metadata": {},
   "source": [
    "#### Compare models using a timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "238f8db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1b9d48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5087 entries, 0 to 5086\n",
      "Columns: 3198 entries, LABEL to FLUX.3197\n",
      "dtypes: float64(3197), int64(1)\n",
      "memory usage: 124.1 MB\n",
      "\n",
      "Run Time: 0.014196634292602539 seconds.\n"
     ]
    }
   ],
   "source": [
    "# try an example:\n",
    "start = time.time()\n",
    "df.info()\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "\n",
    "print('\\nRun Time: ' + str(elapsed) + ' seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bbc08e",
   "metadata": {},
   "source": [
    "####  Compare GradientBoostingClassifier and XGBoostClassifier with the exoplanet dataset for its speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a274ba4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9874213836477987\n",
      "\n",
      "Run Time: 162.29657530784607 seconds\n"
     ]
    }
   ],
   "source": [
    "#GradientBoostingClassifier:\n",
    "start = time.time()\n",
    "gbr = GradientBoostingClassifier(n_estimators=100, max_depth=2, random_state=2)\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "print('Score: ' + str(score))\n",
    "end = time.time()\n",
    "\n",
    "elapsed = end - start\n",
    "print('\\nRun Time: ' + str(elapsed) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de2fb404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:00:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Score: 0.9913522012578616\n",
      "Run Time: 6.5532145500183105 seconds\n"
     ]
    }
   ],
   "source": [
    "# XGBClassifier:\n",
    "start = time.time()\n",
    "xg_reg = XGBClassifier(n_estimators=100, max_depth=2, random_state=2)\n",
    "xg_reg.fit(X_train, y_train)\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "\n",
    "print('Score: ' + str(score))\n",
    "end = time.time()\n",
    "\n",
    "elapsed = end - start\n",
    "print('Run Time: ' + str(elapsed) + ' seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
